{% raw %}
groups:
  - name: alert.rules
    rules:

      ## Availability

      - alert: InstanceDown
        expr: up == 0
        for: 20m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} down"
          description: "{{ $labels.instance }} of job {{ $labels.job }} is down."

      ## CPU

      - alert: CPUHigh80Percent
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle", job!="data-registry"}[5m])) * 100) >= 80
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has greater than 80% CPU usage"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has greater than 80% CPU usage."

      - alert: CPUHigh80PercentRegistry
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle", job="data-registry"}[5m])) * 100) >= 80
        for: 8h
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has greater than 80% CPU usage"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has greater than 80% CPU usage."

      ## Memory

      - alert: AvailableMemoryLow90PerCent
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has less than 10% available memory"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has less than 10% available memory."

      ## Disk IO

      - alert: DiskIOHigh20Percent
        expr: (avg by(instance, device) (rate(node_disk_io_time_seconds_total{job!~"kingfisher.*|data-registry"}[10m]))) * 100 >= 20
        for: 4h
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{$labels.instance}} has greater than 20% disk utilisation on {{ $labels.device }}"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has greater than 20% disk utilisation on {{ $labels.device }}."

      - alert: DiskIOHigh20PercentKingfisher
        expr: (avg by(instance, device) (rate(node_disk_io_time_seconds_total{job=~"kingfisher.*"}[10m]))) * 100 >= 30
        for: 12h
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{$labels.instance}} has greater than 30% disk utilisation on {{ $labels.device }}"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has greater than 30% disk utilisation on {{ $labels.device }}."

      - alert: DiskIOHigh20PercentRegistry
        expr: (avg by(instance, device) (rate(node_disk_io_time_seconds_total{job=~"data-registry"}[10m]))) * 100 >= 20
        for: 12h
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{$labels.instance}} has greater than 20% disk utilisation on {{ $labels.device }}"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has greater than 20% disk utilisation on {{ $labels.device }}."

      ## Disk space

      - alert: RootFileSystemLow
        expr: node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} < 0.15
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has less than 15% of disk space"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has less than 15% of disk space."

      - alert: LowDiskSpace15Percent
        expr:
          node_filesystem_avail_bytes{fstype!~"rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs|aufs|proc", mountpoint!~".*(/aufs|docker).*"}
          / node_filesystem_size_bytes{fstype!~"rootfs|selinuxfs|autofs|rpc_pipefs|tmpfs|aufs|proc", mountpoint!~".*(/aufs|docker).*"}
          * 100 <= 15
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{$labels.instance}} has less than 15% of disk space"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has less than 15% of disk space."

      ## Hardware server, Disk SMART monitoring

      # Alerting when raw value > 0 https://www.backblaze.com/blog/hard-drive-smart-stats/

      - alert: SmartMonDeviceIsUnhealthy
        expr: smartmon_device_smart_healthy != 1
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} is reporting that a physical hard disk is unhealthy"
          description: "{{ $labels.instance }} of job {{ $labels.job }} is reporting that a physical hard disk is unhealthy."

      - alert: SmartMonDeviceLifeLeftLow
        expr: smartmon_wear_leveling_count_value < smartmon_wear_leveling_count_threshold
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} is reporting that a physical hard disk has less than 50% expected life left"
          description: "{{ $labels.instance }} of job {{ $labels.job }} is reporting that a physical hard disk has less than 50% expected life left."

      # Alerts for anything extra we add via the node exporter textfile directory feature.
      # One needs to be set up for every file we are expecting.

      - alert: SmartMonDataIsMissing
        expr: absent(node_textfile_mtime_seconds{file="/home/prometheus-client/node-exporter-textfile-directory/smartmon.sh.prom"})
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} does not have data for Smartmon"
          description: "{{ $labels.instance }} of job {{ $labels.job }} is reporting it does not have data for Smartmon"

      - alert: SmartReallocatedSectorCount
        expr: smartmon_reallocated_sector_ct_raw_value > 0
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has greater than 0 SMART Reallocated Sector Count"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has greater than 0 SMART Reallocated Sector Count"

      - alert: SmartReportedUncorrect
        expr: smartmon_reported_uncorrect_raw_value > 0
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has greater than 0 SMART Reported Uncorrect"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has greater than 0 SMART Reported Uncorrect"

      - alert: SmartCurrentPendingSector
        expr: smartmon_current_pending_sector_raw_value > 0
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has greater than 0 SMART Current Pending Sector"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has greater than 0 SMART Current Pending Sector"

      # Alerting when normalised value < 10 https://image-us.samsung.com/SamsungUS/b2b/resource/2016/05/31/WHP-SSD-SSDSMARTATTRIBUTES-APR16J.pdf
      - alert: SmartUnusedReservedBlockCountTotal
        expr: smartmon_unused_rsvd_blk_cnt_tot_value < smartmon_unused_rsvd_blk_cnt_tot_threshold
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has less than 10% SMART Unused Reserved Block Count Total"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has less than 10% SMART Unused Reserved Block Count Total"

      - alert: SmartProgramFailCountTotal
        expr: smartmon_program_fail_cnt_total_value < smartmon_program_fail_cnt_total_threshold
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has less than 10% SMART Program Fail Count Total"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has less than 10% SMART Program Fail Count Total"

      - alert: SmartEraseFailCountTotal
        expr: smartmon_erase_fail_count_total_value < smartmon_erase_fail_count_total_threshold
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has less than 10% SMART Erase Fail Count Total"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has less than 10% SMART Erase Fail Count Total"

      - alert: SmartRuntimeBadBlock
        expr: smartmon_runtime_bad_block_value < smartmon_runtime_bad_block_threshold
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} has less than 10% SMART Runtime Bad Block"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has less than 10% SMART Runtime Bad Block"
{% endraw %}
